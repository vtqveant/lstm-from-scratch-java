1. Градиент для оценки (посчитанной по матрице) wrt context vectors, т.е. векторов, которые будут строиться на
   основе attn. и кодировать term graph. Это та часть, которая должна отрабатывать после декодера в финальной системе.
   Здесь векторные репрезентации синтаксических категорий считаются данными заранее и неизменными в процессе обучения.
   Здесь оптимизируется только сама оценка, в финальной системе будет margin ranking loss, который
   будет считать оценку для корректных и испорченных предложений.
   Это сделать сперва на Java, чтобы отладить функцию расчета оценки и посмотреть, как она себя ведет.

2. Портировать часть 1 на Torch.

3. Взять seq2seq из OpenNMT и встроить туда оценку. Реализовать margin ranking loss.


---

8 ноября (ср)

  * оптимизатор (BPTT) для encoder-decoder (с эпохой и сохранением encoder, очистка значений в encoder, а декодер достраивается на лету)

9 ноября (чт)

  * убрать ненужные копирования параметров
  * average loss accross mini-batch ???
  * обучить seq2seq на нескольких примерах LCG (подготовленных вручную) -- проверка механизма декомпозиции (только фрейм, без линковки; пока без attention, т.е. без proper alignment)
  * сохранение и загрузка обученной модели (параметры и архитектура)
  * attention для encoder-decoder (достраивать unrolled encoder-decoder network и обучать с помощью BPTT-оптимизатора) -- это decomposition attention
  * визуализатор для построенного alignment
  * normalize input
  * gradient clipping
  * embedding layer (сейчас сразу one-hot)
  * сохранять метаинформацию в snapshot

 10 ноября (пт)

  * attention для линковки
  * score для линковки
  * паспорт

11 ноябра (сб)

  * разметить руками несколько диалогов из DBDC3 (по классификации пользовательских стратегий), обучить linkage

12 ноября (вс)

  * evaluation по DBDC3 (?)
  * слайды